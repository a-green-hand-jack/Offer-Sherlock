#!/usr/bin/env python3
"""Test LLM extraction with Tencent job data."""

import asyncio
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from pydantic import BaseModel, Field
from typing import Optional, List

from offer_sherlock.crawlers import OfficialCrawler
from offer_sherlock.llm import LLMClient, LLMProvider


class JobListing(BaseModel):
    """Extracted job listing information."""
    title: str = Field(description="èŒä½åç§°")
    department: Optional[str] = Field(default=None, description="éƒ¨é—¨/äº‹ä¸šç¾¤")
    experience: Optional[str] = Field(default=None, description="å·¥ä½œç»éªŒè¦æ±‚")
    location: Optional[str] = Field(default=None, description="å·¥ä½œåœ°ç‚¹")
    job_type: Optional[str] = Field(default=None, description="æ‹›è˜ç±»å‹ï¼ˆç¤¾æ‹›/æ ¡æ‹›/å®ä¹ ï¼‰")
    description: Optional[str] = Field(default=None, description="èŒä½æè¿°æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰")
    update_date: Optional[str] = Field(default=None, description="æ›´æ–°æ—¥æœŸ")


class ExtractedJobs(BaseModel):
    """Extracted job listings from page."""
    company: str = Field(description="å…¬å¸åç§°")
    total_positions: int = Field(description="é¡µé¢æ˜¾ç¤ºçš„èŒä½æ€»æ•°")
    jobs: List[JobListing] = Field(description="æå–çš„èŒä½åˆ—è¡¨")


async def main():
    """Test extraction pipeline."""
    print("ğŸ•·ï¸ + ğŸ¤– Tencent Job Extraction Test\n")

    # Read saved markdown
    markdown_file = "data/mock/tencent_positions.md"
    if os.path.exists(markdown_file):
        with open(markdown_file, "r") as f:
            markdown_content = f.read()
        print(f"Loaded cached markdown: {len(markdown_content)} chars")
    else:
        # Crawl fresh
        print("Crawling Tencent positions...")
        crawler = OfficialCrawler(verbose=True)
        result = await crawler.crawl(
            "https://careers.tencent.com/search.html?pcid=40001",
            timeout=90000,
        )
        markdown_content = result.markdown
        print(f"Crawled: {len(markdown_content)} chars")

    # Initialize LLM
    llm = LLMClient(provider=LLMProvider.QWEN)
    print(f"LLM: {llm}\n")

    # Extract jobs
    print("=" * 60)
    print("Extracting jobs with LLM...")
    print("=" * 60)

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›è˜ä¿¡æ¯æå–åŠ©æ‰‹ã€‚
è¯·ä»ç½‘é¡µå†…å®¹ä¸­æå–èŒä½ä¿¡æ¯ã€‚æ³¨æ„ï¼š
1. ä»”ç»†è¯†åˆ«æ¯ä¸ªèŒä½çš„æ ‡é¢˜ã€éƒ¨é—¨ã€ç»éªŒè¦æ±‚ã€åœ°ç‚¹ç­‰ä¿¡æ¯
2. èŒä½æè¿°è¯·ç®€è¦æ¦‚æ‹¬ï¼ˆ50å­—ä»¥å†…ï¼‰
3. æå–é¡µé¢æ˜¾ç¤ºçš„æ‰€æœ‰èŒä½ï¼ˆæœ€å¤š10ä¸ªï¼‰
4. å¦‚æœæŸäº›å­—æ®µæ— æ³•ç¡®å®šï¼Œè®¾ä¸º null"""

    # Use relevant portion of markdown
    relevant_content = markdown_content[-5000:]  # Job listings are usually at the end

    user_message = f"""è¯·ä»ä»¥ä¸‹è…¾è®¯æ‹›è˜é¡µé¢å†…å®¹ä¸­æå–èŒä½ä¿¡æ¯ï¼š

---
{relevant_content}
---

è¯·æå–æ‰€æœ‰èƒ½æ‰¾åˆ°çš„èŒä½ä¿¡æ¯ã€‚"""

    try:
        extracted = llm.chat_structured(
            user_message,
            output_schema=ExtractedJobs,
            system_prompt=system_prompt,
        )

        print(f"\nâœ… Extraction successful!")
        print(f"\nCompany: {extracted.company}")
        print(f"Total positions on page: {extracted.total_positions}")
        print(f"\nExtracted {len(extracted.jobs)} jobs:\n")

        for i, job in enumerate(extracted.jobs, 1):
            print(f"[{i}] {job.title}")
            print(f"    Department: {job.department}")
            print(f"    Experience: {job.experience}")
            print(f"    Location: {job.location}")
            print(f"    Type: {job.job_type}")
            print(f"    Updated: {job.update_date}")
            if job.description:
                print(f"    Description: {job.description}")
            print()

    except Exception as e:
        print(f"âŒ Extraction failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("Please set DASHSCOPE_API_KEY")
        sys.exit(1)

    asyncio.run(main())
