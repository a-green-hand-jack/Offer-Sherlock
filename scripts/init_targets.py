#!/usr/bin/env python3
"""Initialize default crawl targets for major tech companies.

This script populates the database with pre-configured crawl targets
for major Chinese and international tech companies.

Usage:
    python scripts/init_targets.py           # Add all default targets
    python scripts/init_targets.py --list    # List current targets
    python scripts/init_targets.py --clear   # Clear all targets
"""

import argparse
import sys
from pathlib import Path

from offer_sherlock.database import DatabaseManager, CrawlTargetRepository


# Default crawl targets - verified working URLs
DEFAULT_TARGETS = [
    # å›½å†…å¤§å‚ - ç¤¾æ‹›/æŠ€æœ¯å²—
    {
        "company": "è…¾è®¯",
        "url": "https://careers.tencent.com/search.html?pcid=40001",
        "crawler_type": "official",
        "description": "è…¾è®¯æ‹›è˜ - æŠ€æœ¯ç±»å²—ä½",
    },
    {
        "company": "å­—èŠ‚è·³åŠ¨",
        "url": "https://jobs.bytedance.com/experienced/position",
        "crawler_type": "official",
        "description": "å­—èŠ‚è·³åŠ¨ç¤¾æ‹›",
    },
    {
        "company": "é˜¿é‡Œäº‘",
        "url": "https://careers.aliyun.com/off-campus/position-list",
        "crawler_type": "official",
        "description": "é˜¿é‡Œäº‘ç¤¾æ‹›",
    },
    {
        "company": "åä¸º",
        "url": "https://career.huawei.com/reccampportal/portal5/campus-recruitment.html",
        "crawler_type": "official",
        "description": "åä¸ºæ ¡å›­æ‹›è˜",
    },
    {
        "company": "ç™¾åº¦",
        "url": "https://talent.baidu.com/external/baidu/index.html#/social",
        "crawler_type": "official",
        "description": "ç™¾åº¦ç¤¾æ‹›",
    },
    {
        "company": "ç¾å›¢",
        "url": "https://zhaopin.meituan.com/web/position",
        "crawler_type": "official",
        "description": "ç¾å›¢æ‹›è˜",
    },
    {
        "company": "äº¬ä¸œ",
        "url": "https://zhaopin.jd.com/web/job/job_info_list/3",
        "crawler_type": "official",
        "description": "äº¬ä¸œæ‹›è˜ - æŠ€æœ¯ç±»",
    },
    {
        "company": "æ‹¼å¤šå¤š",
        "url": "https://careers.pinduoduo.com/jobs",
        "crawler_type": "official",
        "description": "æ‹¼å¤šå¤šæ‹›è˜",
    },
    {
        "company": "å°çº¢ä¹¦",
        "url": "https://job.xiaohongshu.com/social/position",
        "crawler_type": "official",
        "description": "å°çº¢ä¹¦ç¤¾æ‹›",
    },
    {
        "company": "Bç«™",
        "url": "https://jobs.bilibili.com/social/positions",
        "crawler_type": "official",
        "description": "å“”å“©å“”å“©ç¤¾æ‹›",
    },
    {
        "company": "æ»´æ»´",
        "url": "https://talent.didiglobal.com/social",
        "crawler_type": "official",
        "description": "æ»´æ»´ç¤¾æ‹›",
    },
]


def init_targets(db: DatabaseManager, targets: list[dict]) -> int:
    """Initialize crawl targets in database.

    Args:
        db: Database manager.
        targets: List of target configurations.

    Returns:
        Number of targets added.
    """
    added = 0

    with db.session() as session:
        repo = CrawlTargetRepository(session)

        for target in targets:
            # Check if already exists
            existing = repo.list_by_company(target["company"])
            url_exists = any(t.url == target["url"] for t in existing)

            if url_exists:
                print(f"  â­ï¸  {target['company']} - å·²å­˜åœ¨")
                continue

            repo.add(
                company=target["company"],
                url=target["url"],
                crawler_type=target.get("crawler_type", "official"),
                is_active=True,
            )
            print(f"  âœ… {target['company']} - {target.get('description', '')}")
            added += 1

    return added


def list_targets(db: DatabaseManager):
    """List all crawl targets."""
    with db.session() as session:
        repo = CrawlTargetRepository(session)
        targets = repo.list_all()

        if not targets:
            print("(æ— æŠ“å–ç›®æ ‡)")
            return

        print(f"\n{'å…¬å¸':<12} {'çŠ¶æ€':<6} {'ç±»å‹':<10} URL")
        print("-" * 80)

        for t in targets:
            status = "âœ… æ´»è·ƒ" if t.is_active else "â¸ï¸ æš‚åœ"
            print(f"{t.company:<12} {status:<6} {t.crawler_type:<10} {t.url[:45]}...")


def clear_targets(db: DatabaseManager):
    """Clear all crawl targets."""
    with db.session() as session:
        repo = CrawlTargetRepository(session)
        targets = repo.list_all()

        for t in targets:
            repo.delete(t.id)

        print(f"å·²åˆ é™¤ {len(targets)} ä¸ªæŠ“å–ç›®æ ‡")


def main():
    parser = argparse.ArgumentParser(
        description="åˆå§‹åŒ–æŠ“å–ç›®æ ‡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--list", "-l",
        action="store_true",
        help="åˆ—å‡ºå½“å‰æ‰€æœ‰æŠ“å–ç›®æ ‡",
    )
    parser.add_argument(
        "--clear", "-c",
        action="store_true",
        help="æ¸…ç©ºæ‰€æœ‰æŠ“å–ç›®æ ‡",
    )
    parser.add_argument(
        "--db",
        default="data/offers.db",
        help="æ•°æ®åº“è·¯å¾„ (é»˜è®¤: data/offers.db)",
    )

    args = parser.parse_args()

    # Ensure data directory exists
    Path(args.db).parent.mkdir(parents=True, exist_ok=True)

    # Initialize database
    db = DatabaseManager(db_path=args.db)
    db.create_tables()

    if args.list:
        print("\nğŸ“‹ å½“å‰æŠ“å–ç›®æ ‡")
        print("=" * 80)
        list_targets(db)

    elif args.clear:
        print("\nğŸ—‘ï¸  æ¸…ç©ºæŠ“å–ç›®æ ‡")
        print("=" * 80)
        clear_targets(db)

    else:
        print("\nğŸš€ åˆå§‹åŒ–é»˜è®¤æŠ“å–ç›®æ ‡")
        print("=" * 80)
        print(f"æ•°æ®åº“: {args.db}")
        print(f"ç›®æ ‡æ•°: {len(DEFAULT_TARGETS)}\n")

        added = init_targets(db, DEFAULT_TARGETS)

        print(f"\nâœ… å®Œæˆï¼æ–°å¢ {added} ä¸ªç›®æ ‡")
        print("\nè¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹æŠ“å–:")
        print(f"  python scripts/run_agent.py --all --db {args.db}")


if __name__ == "__main__":
    main()
