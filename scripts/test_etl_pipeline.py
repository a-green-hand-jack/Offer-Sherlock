#!/usr/bin/env python3
"""Integration test for the complete ETL pipeline.

Tests the full flow:
1. Extract: Crawl official sites and XHS
2. Transform: Use LLM to extract structured data
3. Load: Persist to SQLite database
"""

import asyncio
from pathlib import Path

from offer_sherlock.crawlers import OfficialCrawler, XhsCrawler
from offer_sherlock.extractors import JobExtractor, InsightExtractor
from offer_sherlock.database import (
    DatabaseManager,
    JobRepository,
    InsightRepository,
    CrawlTargetRepository,
)
from offer_sherlock.llm.client import LLMClient
from offer_sherlock.utils.config import LLMProvider


async def test_job_etl():
    """Test ETL for official job postings."""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• Job ETL Pipeline")
    print("=" * 60)

    # Setup database
    db = DatabaseManager(db_path="data/test_etl.db")
    db.create_tables()

    # Use existing crawl results
    data_dir = Path(__file__).parent.parent / "data" / "crawl_results"
    google_file = data_dir / "Google.md"

    if not google_file.exists():
        print("âŒ æœªæ‰¾åˆ° Google.mdï¼Œè·³è¿‡æµ‹è¯•")
        return

    print(f"\nğŸ“– è¯»å–: {google_file.name}")
    content = google_file.read_text()[:12000]

    # Extract
    print("ğŸ¤– ä½¿ç”¨ Qwen-plus æå–å²—ä½ä¿¡æ¯...")
    llm = LLMClient(provider=LLMProvider.QWEN, model="qwen-plus")
    extractor = JobExtractor(llm_client=llm)

    result = await extractor.extract(
        content=content,
        company="Google",
        source_url="https://careers.google.com",
    )

    print(f"âœ… æå–åˆ° {result.count} ä¸ªå²—ä½")

    # Load to database
    print("\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
    with db.session() as session:
        repo = JobRepository(session)

        # Add crawl target
        target_repo = CrawlTargetRepository(session)
        target_repo.add(
            company="Google",
            url="https://careers.google.com",
            crawler_type="official",
        )

        # Add jobs
        jobs = repo.add_many(result.jobs, source_url=result.source_url)
        print(f"âœ… ä¿å­˜äº† {len(jobs)} ä¸ªå²—ä½")

        # Query back
        google_jobs = repo.list_by_company("Google")
        print(f"\nğŸ“Š æ•°æ®åº“ä¸­ Google å²—ä½æ•°: {len(google_jobs)}")

        for job in google_jobs[:3]:
            print(f"  - {job.title} [{job.location or 'æœªçŸ¥'}]")


async def test_insight_etl():
    """Test ETL for social insights."""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• Insight ETL Pipeline")
    print("=" * 60)

    # Setup database
    db = DatabaseManager(db_path="data/test_etl.db")
    db.create_tables()

    # Search XHS
    print("\nğŸ” æœç´¢å°çº¢ä¹¦: è…¾è®¯ offer")

    async with XhsCrawler(headless=True) as crawler:
        notes = await crawler.search("è…¾è®¯ offer", max_results=5)

        if not notes:
            print("âŒ æœªæ‰¾åˆ°ç¬”è®°ï¼Œè·³è¿‡æµ‹è¯•")
            return

        print(f"ğŸ“– æ‰¾åˆ° {len(notes)} æ¡ç¬”è®°")

        # Extract
        print("\nğŸ¤– ä½¿ç”¨ Qwen-plus åˆ†æç¬”è®°...")
        llm = LLMClient(provider=LLMProvider.QWEN, model="qwen-plus")
        extractor = InsightExtractor(llm_client=llm)

        summary = await extractor.analyze_notes(
            notes=notes,
            company="è…¾è®¯",
            position_keyword="offer",
        )

        print(f"âœ… ç”Ÿæˆæƒ…æŠ¥æ±‡æ€»: {summary.overall_sentiment.value}")

        # Load to database
        print("\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
        with db.session() as session:
            repo = InsightRepository(session)
            insight = repo.add(summary)
            print(f"âœ… ä¿å­˜æƒ…æŠ¥ ID: {insight.id}")
            print(f"   å…³è”å¸–å­æ•°: {len(insight.social_posts)}")

            # Query back
            latest = repo.get_latest_by_company("è…¾è®¯")
            if latest:
                print(f"\nğŸ“Š æœ€æ–°æƒ…æŠ¥:")
                print(f"   å…¬å¸: {latest.company}")
                print(f"   å…³é”®è¯: {latest.position_keyword}")
                print(f"   è–ªèµ„ä¼°ç®—: {latest.salary_estimate or 'æœªçŸ¥'}")
                print(f"   é¢è¯•éš¾åº¦: {latest.interview_difficulty or 'æœªçŸ¥'}")
                print(f"   ç»¼åˆè¯„ä»·: {latest.overall_sentiment}")


async def show_database_stats():
    """Show database statistics."""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡")
    print("=" * 60)

    db = DatabaseManager(db_path="data/test_etl.db")

    with db.session() as session:
        job_repo = JobRepository(session)
        insight_repo = InsightRepository(session)
        target_repo = CrawlTargetRepository(session)

        print(f"\næ€»å²—ä½æ•°: {job_repo.count()}")
        print(f"æ€»æƒ…æŠ¥æ•°: {insight_repo.count()}")
        print(f"æŠ“å–ç›®æ ‡æ•°: {len(target_repo.list_all())}")

        # List companies
        from sqlalchemy import select, distinct
        from offer_sherlock.database.models import Job

        stmt = select(distinct(Job.company))
        companies = list(session.scalars(stmt))
        print(f"\nå·²æ”¶å½•å…¬å¸: {', '.join(companies)}")


async def main():
    """Run full ETL integration test."""
    print("ğŸš€ Offer-Sherlock ETL Pipeline é›†æˆæµ‹è¯•")
    print("   å®Œæ•´æµç¨‹: Extract â†’ Transform â†’ Load\n")

    try:
        await test_job_etl()
    except Exception as e:
        print(f"âŒ Job ETL å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    try:
        await test_insight_etl()
    except Exception as e:
        print(f"âŒ Insight ETL å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    await show_database_stats()

    print("\nâœ… ETL é›†æˆæµ‹è¯•å®Œæˆ")
    print(f"   æ•°æ®åº“æ–‡ä»¶: data/test_etl.db")


if __name__ == "__main__":
    asyncio.run(main())
